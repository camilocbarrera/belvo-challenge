{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# ETL PROCESS\n",
    "\n",
    "Database (Snowflake DW)\n",
    "This Ingest process uses the Snowflake data warehouse to hold the external data.\n",
    "“Snowflake is an analytics data warehouse delivered as software as a service (SaaS). Snowflake’s data warehouse does not rely on an existing database or “big data” software platform like Hadoop. Snowflake’s data warehouse uses a new SQL database engine with a unique architecture designed for the cloud.”\n",
    "For more information: https://www.snowflake.com/workloads/data-warehouse-modernization/"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Definition of functions\n",
    "The necessary functions for the ETL process will be defined below.\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Warehouse Functions\n",
    "Next we will define the functions that will allow us to interact between Python and our Warehouse\n",
    "Connection for Snowflake DW\n",
    "Note: Create venv file with variable user and password, this for security of our data.\n",
    "This is a Personal warehouse, but the database and schemas have been isolated to work in this specific case."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [],
   "source": [
    "import snowflake.connector\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "import pandas as pd\n",
    "from snowflake.connector.pandas_tools import write_pandas\n",
    "\n",
    "\"\"\"\n",
    "Set up connection\n",
    "\"\"\"\n",
    "load_dotenv()\n",
    "con = snowflake.connector.connect(\n",
    "    user=os.getenv('SNOWFLAKE_USER'),\n",
    "    password=os.getenv('SNOWFLAKE_PASSWORD'),\n",
    "    account=os.getenv('SNOWFLAKE_ACCOUNT'),\n",
    "    warehouse=os.getenv('SNOWFLAKE_WAREHOUSE'),\n",
    "    database='BELVO_ANALYTICS',\n",
    "    schema='RAW',\n",
    "\n",
    ")\n",
    "\n",
    "\n",
    "def get_data_from_snowflake(sql: str):\n",
    "    \"\"\"\n",
    "    Function for get data from Snowflake, from a qwuery string\n",
    "    :param sql: Query String\n",
    "    :return: parsing Dataframe\n",
    "    \"\"\"\n",
    "    cursor = con.cursor()\n",
    "    cursor.execute(sql)\n",
    "    result = cursor.fetch_pandas_all()\n",
    "    return result\n",
    "\n",
    "\n",
    "def map_cols_sf(df) -> str:\n",
    "    \"\"\"\n",
    "    :param df:\n",
    "    :return: columns remaping\n",
    "    \"\"\"\n",
    "    columns_sql = \"\"\n",
    "    for column in df.columns:\n",
    "        if (df[column].dtype.name == \"int\" or df[column].dtype.name == \"int64\"):\n",
    "            columns_sql = columns_sql + column + \" int\"\n",
    "        elif df[column].dtype.name == \"object\":\n",
    "            columns_sql = columns_sql + column + \" varchar(16777216)\"\n",
    "        elif df[column].dtype.name == \"datetime64[ns]\":\n",
    "            columns_sql = columns_sql + column + \" datetime\"\n",
    "        elif df[column].dtype.name == \"float64\":\n",
    "            columns_sql = columns_sql + column + \" float8\"\n",
    "        elif df[column].dtype.name == \"bool\":\n",
    "            columns_sql = columns_sql + column + \" boolean\"\n",
    "        else:\n",
    "            columns_sql = columns_sql + column + \" varchar(16777216)\"\n",
    "        if df[column].name != df.columns[-1]:\n",
    "            columns_sql = columns_sql + \",\\n\"\n",
    "\n",
    "    return columns_sql\n",
    "\n",
    "\n",
    "def send_data_to_snowflake(df, schema: str, name: str, hard_create, quote_identifiers=False):\n",
    "    \"\"\"\n",
    "    Function for send data to Snowflake from a data Frame\n",
    "    :param quote_identifiers: Optional for column String with space or special char\n",
    "    :param hard_create: Optional to force creation of a table\n",
    "    :param df: Input Data Frame\n",
    "    :param schema: Schema to aim our table\n",
    "    :param name: Table name\n",
    "    \"\"\"\n",
    "\n",
    "    df_1 = df.copy()\n",
    "\n",
    "    map_columns = map_cols_sf(df_1)\n",
    "\n",
    "    ddl_create = f\"CREATE OR REPLACE TABLE {schema.upper()}.{name.upper()} ({map_columns})\"\n",
    "    con.cursor().execute(ddl_create)\n",
    "\n",
    "    write_pandas(con, df_1, table_name=name.upper(), schema=schema.upper(), quote_identifiers=quote_identifiers)\n",
    "\n",
    "\n",
    "def put_data_in_snowflake( df, schema: str, table_name: str, ):\n",
    "    df_1 = df.copy()\n",
    "    return write_pandas(con, df_1, table_name=table_name.upper(), schema=schema.upper(), auto_create_table=True)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Functions to handling file"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "outputs": [],
   "source": [
    "\n",
    "def csv_to_df(filename):\n",
    "    \"\"\"\n",
    "    Function for read csv File\n",
    "    :param filename:\n",
    "    :return: Dataframe parsing from CSV\n",
    "    \"\"\"\n",
    "    df = pd.read_csv(filename, sep=',', encoding='UTF8')\n",
    "    return df\n",
    "\n",
    "\n",
    "def standardize_columns(df):\n",
    "    \"\"\"\n",
    "    Function for standardize columns of a dataframe\n",
    "    :param df:\n",
    "    :return: Columns standardized\n",
    "    \"\"\"\n",
    "\n",
    "    df_1 = df.copy()\n",
    "    new_columns = []\n",
    "    count = 1\n",
    "    for x in df_1.columns:\n",
    "        x.lower()\n",
    "        mt = x.lower().maketrans(\"/'.,:¿?/()ÁÉÍÓÚáéíóú\", '          aeiouaeiou')\n",
    "        new_column = x.translate(mt).strip().replace(\" \", \"_\").upper()\n",
    "        new_columns.append(new_column)\n",
    "\n",
    "    return new_columns\n",
    "\n",
    "\n",
    "def get_columns_duplicated(df):\n",
    "    \"\"\"\n",
    "    Function to get a list of columns duplicated in a dataframe\n",
    "    :param df:\n",
    "    :return: List of columns duplicated in a dataframe\n",
    "    \"\"\"\n",
    "    duplicated_columns = []\n",
    "    for c in df.columns:\n",
    "\n",
    "        if df.columns.to_list().count(c) > 1:\n",
    "            duplicated_columns.append(c)\n",
    "\n",
    "    return list(set(duplicated_columns))\n",
    "\n",
    "\n",
    "def rename_duplicated_columns(df):\n",
    "    \"\"\"\n",
    "    Function for rename duplicated values in a dataframe\n",
    "    :param df: Input dataframe\n",
    "    :return: Dataframe with renamed columns\n",
    "    \"\"\"\n",
    "    cols = []\n",
    "    count = 1\n",
    "    duplicated_columns = get_columns_duplicated(df)\n",
    "\n",
    "    for duplicated_column in duplicated_columns:\n",
    "        for column in df.columns:\n",
    "            if column == duplicated_column:\n",
    "                cols.append(f'{duplicated_column}_{count}')\n",
    "                count += 1\n",
    "                continue\n",
    "            cols.append(column)\n",
    "        df.columns = cols\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Let's run our pipeline :)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "data": {
      "text/plain": "['contacts.csv',\n 'customers.csv',\n 'companies.csv',\n 'owners.csv',\n 'companies_deals_associations.csv',\n 'contacts_deals_associations.csv',\n 'deals.csv']"
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_names = os.listdir('sources')\n",
    "file_names"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Everything was ok for  contacts  !\n",
      "Everything was ok for  customers  !\n",
      "Everything was ok for  companies  !\n",
      "Everything was ok for  owners  !\n",
      "Everything was ok for  companies_deals_associations  !\n",
      "Everything was ok for  contacts_deals_associations  !\n",
      "Everything was ok for  deals  !\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for file in file_names:\n",
    "\n",
    "    #1.  Read local File\n",
    "    file_name = f\"sources/{file}\"\n",
    "    df_raw = csv_to_df(file_name)\n",
    "\n",
    "    #2. Handling column names\n",
    "    df_raw.columns = standardize_columns(df_raw)\n",
    "\n",
    "    #3. Check and handling duplicated columns\n",
    "    rename_duplicated_columns(df_raw)\n",
    "    table_name = file.replace('.csv','')\n",
    "\n",
    "    #4. Send raw data to Snowflake :3\n",
    "    put_data_in_snowflake(df_raw, 'RAW',table_name )\n",
    "    print(\"Everything was ok for table\", table_name, \" !\")\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
